{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Dask on AzureML\n",
    "\n",
    "This notebook shows how to run a Dask cluster on an AzureML Compute cluster. \n",
    "For setup instructions of you python environment, please see the [Readme](../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import VERSION\n",
    "import uuid\n",
    "import time\n",
    "VERSION\n",
    "\n",
    "# some helper to generate URLs later\n",
    "class DaskURLs:\n",
    "    def __init__(self, bokeh_port, jupyter_port, jupyter_token):\n",
    "        self.bokeh_port = bokeh_port\n",
    "        self.jupyter_port = jupyter_port\n",
    "        self.jupyter_token = jupyter_token\n",
    "        \n",
    "    def _repr_javascript_(self):\n",
    "        return f'''\n",
    "        var hostname = window.location.hostname\n",
    "        var dot = hostname.indexOf('.')\n",
    "        var first = hostname.substr(0, dot)\n",
    "        var last = hostname.substr(dot)\n",
    "        var bokeh = 'https://' + first +'-{self.bokeh_port}'+ last\n",
    "        var jupyter = 'https://' + first +'-{self.jupyter_port}'+ last+'?token={self.jupyter_token}'\n",
    "        element.html(`\n",
    "            Bokeh: <a href=`+bokeh+` target='bokeh'>`+bokeh+`</a><br>\n",
    "            Jupyter: <a href=`+jupyter+` target='jupyter'>`+jupyter+`</a><br>`)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will get the workspace and AML compute cluster and start the Dask cluster on it. The assumption is that you have created a cluster with the name `dask` -- else change the name below accordingly. **It is important that, as you created the cluster, you have provided a username (I am using `daskuser`) and password and ssh key (ssh key is optional), since you will need to log in to the worker nodes to establish the port forwarding to the docker container.**\n",
    "\n",
    "![create_cluster](../img/create_cluster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D12 v2\t4\t28 GiB\t200 GiB\t$0.379/hour\n",
    "dask_cluster = ws.compute_targets['dask-DS12-V2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Dask cluster using an Estimator with MpiConfiguration. Make sure the cluster is able to scale up to 10 nodes or change the `node_count` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator('dask', \n",
    "                compute_target=dask_cluster, \n",
    "                entry_script='startDask.py', \n",
    "                conda_dependencies_file='environment.yml', \n",
    "                script_params={'--datastore': ws.get_default_datastore()},\n",
    "                node_count=10,\n",
    "                distributed_training=MpiConfiguration())\n",
    "\n",
    "run = Experiment(ws, 'dask').submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while not 'headnode' in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "clear_output()\n",
    "headnode_private_ip = run.get_metrics()['headnode']\n",
    "print('Headnode has IP:', headnode_private_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find the public IP and ssh port of the head node\n",
    "\n",
    "headnode_public_ip = None\n",
    "headnode_ssh_port = None\n",
    "for node in dask_cluster.list_nodes():\n",
    "    if node['privateIpAddress'] == headnode_private_ip:\n",
    "        headnode_public_ip = node['publicIpAddress']\n",
    "        headnode_ssh_port = node['port']\n",
    "        break\n",
    "        \n",
    "if headnode_public_ip == None:\n",
    "    print('Headnode not found in cluster')\n",
    "else:\n",
    "    print(f'Headnode is at {headnode_public_ip}:{headnode_ssh_port}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the port-forwarding from Notebook VM to Dask Scheduler\n",
    "Since Notebook VM does not yet support VNets, you need to build an SSH port forwarder through SSH login.\n",
    "\n",
    "In the prior cell we looked up the public IP and port of the headnode of the cluster \n",
    "\n",
    "Now, open the terminal on the Notebook VM and type what the following cell outputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ssh daskuser@{headnode_public_ip} -p {headnode_ssh_port} -L 8786:localhost:8786 -L 8788:{headnode_private_ip}:8787 -L 9999:localhost:8888')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to leave the terminal tab open to keep the port-forward running\n",
    "\n",
    "As you see, you are forwarding 3 ports \n",
    "\n",
    "1. 8786 is for the scheduler and will be used to connect the client to the cluster\n",
    "2. 8788 is for the Bokeh app that shows the activity on the cluster (we are mapping to the local port 8788 to avoid a conflict with the RStudio Server running on the Notebook VM)\n",
    "3. 9999 is for a jupyter instance running on the head node. You can connect to the scheduler from the jupyter running on your Notebook VM or from this jupyter instance on the head node.   \n",
    "\n",
    "To access the Bokeh app, change the URL to your notebook VM by adding `-8788` right after the machine name. If you are running this notebook on a Notebook VM, then you can create the URLs by excuting the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for jupyter token\")\n",
    "while not 'jupyter-token' in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "# this will only work when running on a Notebook VM\n",
    "DaskURLs('8788', '9999', run.get_metrics()['jupyter-token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you are seeing this after you clicked on the Bokeh link and then select 'Status':\n",
    "\n",
    "![Bokeh](../img/bokeh.png)\n",
    "\n",
    "If you are wondering what all this port business in accomplishing, please see the graph below that tries to illustrate who talks to whom and how.\n",
    "\n",
    "![Network](../img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some jobs on the cluster\n",
    "If you are able to see the Bokeh app, it is time to use the cluster. Thanks to the port forward, the scheduler appears to the notebook VM at `tcp://localhost:8786`. You should see 10 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "c = Client('tcp://localhost:8786')\n",
    "c.restart()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the cluster works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from dask import delayed, visualize\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(abs(np.random.normal(5, 2)))\n",
    "    return x + 1\n",
    "\n",
    "fut = []\n",
    "for i in range(10):\n",
    "    fut.append( c.submit(delayed(inc), i) )\n",
    "\n",
    "fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in fut:\n",
    "    print(i.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(a):\n",
    "    x = 0\n",
    "    for y in a:\n",
    "        x += y\n",
    "    return x\n",
    "\n",
    "results = []\n",
    "for f in fut:\n",
    "    results.append(f.result())\n",
    "    \n",
    "fut2 = c.submit(sum, results)\n",
    "fut2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut2.result().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(fut2.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Large Datasets\n",
    "(from https://github.com/dask/dask-tutorial)\n",
    "\n",
    "Sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on dask arrays and dataframes that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a small (random) dataset locally using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small dataset will be the template for our large random dataset.\n",
    "We'll use `dask.delayed` to adapt `sklearn.datasets.make_blobs`, so that the actual dataset is being generated on our workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 200000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype='float64')\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the array\n",
    "X.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this on the cluster.\n",
    "X = X.persist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans\n",
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut cluster down\n",
    "To shut the cluster down, cancel the job that runs the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments['dask'].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'cancelling run {run.id}')\n",
    "        run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for convenience, get the latest running Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments['dask'].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'latest running run is {run.id}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dask)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
