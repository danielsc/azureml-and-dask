{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a DASK cluster with RAPIDS\n",
    "\n",
    "This notebook runs a DASK cluster with NVIDIA RAPIDS. RAPIDS uses NVIDIA CUDA for high-performance GPU execution, exposing GPU parallelism and high memory bandwidth through a user-friendly Python interface. It includes a dataframe library called cuDF which will be familiar to Pandas users, as well as an ML library called cuML that provides GPU versions of all machine learning algorithms available in Scikit-learn. \n",
    "\n",
    "This notebook shows how through DASK, RAPIDS can take advantage of multi-node, multi-GPU configurations on AzureML. \n",
    "\n",
    "This notebook is deploying the AzureML cluster to a VNet. Prior to running this, setup a VNet and DSVM according to [../setup-vnet.md](../setup-vnet.md). In this case the following names are used to identify the VNet and subnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_hostname='<your-dsvm-DNS-name>.<region>.cloudapp.azure.com'\n",
    "vnet_resourcegroup_name='demo'\n",
    "vnet_name='myvnet'\n",
    "subnet_name='default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import RunConfiguration, MpiConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_cluster_name = \"nd12-vnet-clustr\"\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deply the AmlCompute cluster\n",
    "The next cell is deploying the AmlCompute cluster. The cluster is configured to scale down to 0 nodes after 2 minuten, so no cost is incurred while DASK is not running (and thus no nodes are spun up on the cluster as the result of this cell, yet). This cell only needs to be executed once and the cluster can be reused going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cluster\")\n",
    "\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_ND12s\", \n",
    "        min_nodes=0, \n",
    "        max_nodes=10,\n",
    "        idle_seconds_before_scaledown=120,\n",
    "        vnet_resourcegroup_name=vnet_resourcegroup_name,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name=subnet_name\n",
    "    )\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "\n",
    "    print(\"waiting for nodes\")\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data to Azure Blob Storage\n",
    "\n",
    "This next cell is pulling the NYC taxi data set down and then uploads it to the AzureML workspace's default data store. The all nodes of the DASK cluster we are creating further down will then be able to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-01.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-02.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-03.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-04.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-05.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-06.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-07.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-08.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-09.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-10.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-11.csv\n",
      "- File already exists locally\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-12.csv\n",
      "- File already exists locally\n",
      "- Uploading taxi data... \n",
      "Uploading an estimated of 12 files\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-09.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-10.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-07.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-12.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-06.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-03.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-01.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-02.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-08.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-05.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-04.csv\n",
      "Target already exists. Skipping upload for nyctaxi/yellow_tripdata_2015-11.csv\n",
      "Uploaded 0 files\n",
      "- Data transfer complete\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(cwd, 'data'))\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "taxidir = os.path.join(data_dir, 'nyctaxi')\n",
    "if not os.path.exists(taxidir):\n",
    "    os.makedirs(taxidir)\n",
    "\n",
    "filenames = []\n",
    "local_paths = []\n",
    "for i in range(1, 13):\n",
    "    filename = \"yellow_tripdata_2015-{month:02d}.csv\".format(month=i)\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    local_path = os.path.join(taxidir, filename)\n",
    "    local_paths.append(local_path)\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    url = \"http://dask-data.s3.amazonaws.com/nyc-taxi/2015/\" + filename\n",
    "    print(\"- Downloading \" + url)\n",
    "    if not os.path.exists(local_paths[idx]):\n",
    "        with open(local_paths[idx], 'wb') as file:\n",
    "            with urllib.request.urlopen(url) as resp:\n",
    "                length = int(resp.getheader('content-length'))\n",
    "                blocksize = max(4096, length // 100)\n",
    "                with tqdm(total=length, file=sys.stdout) as pbar:\n",
    "                    while True:\n",
    "                        buff = resp.read(blocksize)\n",
    "                        if not buff:\n",
    "                            break\n",
    "                        file.write(buff)\n",
    "                        pbar.update(len(buff))\n",
    "    else:\n",
    "        print(\"- File already exists locally\")\n",
    "\n",
    "print(\"- Uploading taxi data... \")\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.upload(\n",
    "    src_dir=taxidir,\n",
    "    target_path='nyctaxi',\n",
    "    show_progress=True)\n",
    "\n",
    "print(\"- Data transfer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DASK Cluster\n",
    "\n",
    "On the AMLCompute cluster we are now running a Python job that will run a DASK cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "mpi_config = MpiConfiguration()\n",
    "mpi_config.process_count_per_node = 2\n",
    "\n",
    "est = Estimator(\n",
    "    source_directory='./dask',\n",
    "    compute_target=gpu_cluster,\n",
    "    entry_script='init-dask.py',\n",
    "    script_params={\n",
    "        '--data': ws.get_default_datastore(),\n",
    "        '--gpus': str(2)  # The number of GPUs available on each node\n",
    "        },\n",
    "    node_count=3,\n",
    "    use_gpu=True,\n",
    "    distributed_training=mpi_config,\n",
    "    conda_dependencies_file='rapids-0.9.yaml')\n",
    "\n",
    "run = Experiment(ws, \"init-dask-jupyter\").submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the widget to monitor how the DASK cluster spins up. When run for the first time on a workspace, the following thing will happen:\n",
    "\n",
    "1. The docker image will to be created, which takes about 20 minutes. \n",
    "2. Then AzureML will start to scale the cluster up by provisioning the required number of nodes (`node_count` above), which will take another 5-10 minutes with the chosen Standard_ND12s\n",
    "3. The docker image is being transferred over to the compute nodes, which, given the size of about 8 GB takes another 3-5 minutes\n",
    "\n",
    "So alltogether the process will take up to 30 minutes when run for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b1477cdb6e42809cb4a756ee34f503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headnode has ip:  172.17.0.6\n",
      "Run the following command on your local machine to establish the port forwarding for the dashboard and the Jupyter server\n",
      "\n",
      "     ssh <your-dsvm-DNS-name>.<region>.cloudapp.azure.com -L 8787:172.17.0.6:8787 -L 9999:172.17.0.6:8888\n",
      "\n",
      "\n",
      "Then you should be able to access the following URLs\n",
      "\n",
      "     Dashboard: http://localhost:8787\n",
      "     Jupyter on cluster: http://localhost:9999?token=bb11f5e0e3b537a004944f394ef6901c25788bb592ef557f\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "it = 0\n",
    "while not \"headnode\" in run.get_metrics():\n",
    "    clear_output(wait=True)\n",
    "    print(\"waiting for scheduler node's ip \" + str(it) )\n",
    "    time.sleep(1)\n",
    "    it += 1\n",
    "\n",
    "headnode = run.get_metrics()[\"headnode\"]\n",
    "jupyter_ip = run.get_metrics()[\"jupyter-ip\"]\n",
    "jupyter_port = run.get_metrics()[\"jupyter-port\"]\n",
    "jupyter_token = run.get_metrics()[\"jupyter-token\"]\n",
    "\n",
    "print(\"headnode has ip: \", headnode)\n",
    "print(\"Run the following command on your local machine to establish the port forwarding for the dashboard and the Jupyter server\")\n",
    "print()\n",
    "print(f'     ssh {vm_hostname} -L 8787:{headnode}:8787 -L 9999:{jupyter_ip}:{jupyter_port}')\n",
    "print()\n",
    "print()\n",
    "print(\"Then you should be able to access the following URLs\")\n",
    "print()\n",
    "print(f\"     Dashboard: http://localhost:8787\")\n",
    "print(f\"     Jupyter on cluster: http://localhost:9999?token={jupyter_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutting the cluster down\n",
    "\n",
    "Terminate the run to shut the cluster down. Once you are done with your interactive work, make sure to do this so the AML Compute cluster gets spun down again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last run\n",
    "run = Experiment(ws, \"init-dask-jupyter\").get_runs().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headnode': '172.17.0.7',\n",
       " 'scheduler': '172.17.0.7:8786',\n",
       " 'dashboard': '172.17.0.7:8787',\n",
       " 'data': '/mnt/batch/tasks/shared/LS_root/jobs/vnettest/azureml/init-dask-jupyter_1569881044_1f5cf642/mounts/workspaceblobstore',\n",
       " 'jupyter-url': 'http://172.17.0.7:8888/?token=4e25f4cf857dc8ce541de6e3b8fc4797f6bda4e11ec9ea2c',\n",
       " 'jupyter-port': 8888,\n",
       " 'jupyter-token': '4e25f4cf857dc8ce541de6e3b8fc4797f6bda4e11ec9ea2c',\n",
       " 'jupyter-ip': '172.17.0.7'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rapids-0.9)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
